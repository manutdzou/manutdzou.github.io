---
layout: post
title: Qwen3-RL
category: 科研
tags: 
keywords: 
description:
---

# Qwen3-RL

众所周知，大模型的训练步骤主要包括第一步预训练，第二步SFT，第三步RL。目前主流的开源Instruct模型基本上遵循这条路径，所以我们能得到的开源模型一般是一个经过强化学习后的模型。

对于模型的SFT训练，可以说只要有卡就能进行几乎没有门槛。一般就是针对业务的需求收集相关的数据做好Query-Answer对给到模型训练完成即可。如果我们只需要定向调优这一步足矣。然而在实践中我们发现通过定向调优后的模型能在收集的SFT数据上提升性能，然而对于原本的通用能力有非常明显的损害。

有同学可能会说那就加配比增加通用数据，这样的实验怎么可能被遗漏呢，然而遗憾的是通用能力依旧明显的损害，而且是训练的step越多降低的越厉害。

可能又有同学说肯定是收集的通用数据质量太低，然而事实是即使强如DeepSeek发布的SFT模型DeepSeek-R1-0528-Qwen3-8B，DeepSeek-R1-Distill-Qwen-7B在我们内部构建的指令跟随，KBQA等通用场景任务上都不如基于对应的Instruct模型。另外实验中发现即使拿及其少量的几百条数据用很小很小的learning rate SFT模型几个step，测试后依然能看到显著的整体性能下降。所以再把SFT模型性能下降归咎于数据质量的高低似乎不是很有道理。特别是综合发现不仅仅Qwen模型有这样的现象，Llama也有类似的现象。

所以一定是有本质的原因导致了这样的现象，博主在此给出一种未经可靠验证的猜测。其实Instruct模型从预训练到SFT到RL经历了不同的优化目标。预训练阶段主要是世界知识的注入阶段，这一步也是整个模型最重要的阶段，决定了模型的上限。第二步SFT是指令跟随阶段，通过让模型学习Query-Answer来教会模型遵循Query的要求回答问题。第三步RL则是人类偏好回答对齐。

具体到训练优化目标，第一步和第二步本质上都是next token prediction阶段，优化目标是最小化信息熵，模型只管给定数据的拟合，不管不良路径生成的抑制。而RL阶段一般采用PPO,GRPO这种强化学习算法，优化目标是最大化奖励期望。

在对强化学习阶段的观察可以发现，模型的行为在信息熵上是缓慢增加到一个水平的，很低的信息熵往往预示着RL训练阶段的失败，所以才有了https://arxiv.org/pdf/1909.08593 预设KL target这种人为鼓励增大信息熵的行为。所以可以说RL的优化目标是将模型的信息熵提高到一个水平保持输出的多样性并和人类偏好对齐。这篇文章https://arxiv.org/pdf/1904.09751 似乎也印证了这样的观点，人类的偏好并不总是朝着信息熵最小化的方向生成文字序列的。

所以对于一个Instruct模型，它本身已经收敛到一个比SFT时候信息熵要稍高的阶段，再对它接着SFT压低模型的信息熵，必然导致优化方向偏移，模型性能的下降。

以下，我们基于这些猜想去设计我们的定向调优目标，希望得到一个模型既能保持原有的通用能力不损失，又能在模型欠缺的能力上有极大的提升。所以如果发现模型在定向能力上较弱就接着强化学习，如果模型完全不具备定向能力，可以少量在定向数据上cold start SFT获得能力然后接着强化学习。

因此我们设计了如下一套训练流程，基本上目前的开源RL 框架都有类似的功能。然而为了可定制性和自由度，我从最基础的TRL库出发，借鉴了开源其他RL框架的思想完全手搓了下面的训练框架。

![1](/public/img/posts/RL.png)

