---
layout: post
title: tensorflow的变量初始化和scope
category: tensorflow
tags: 深度学习
keywords: tf学习
description: tf学习
---


# tf.Variable(),tf.get_variable(),tf.Variable_scope(),tf.name_scope()的使用方法

```
tf.get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, custom_getter=None)
```

initializer为变量初始化的方法，初始化有如下几种：

```
tf.constant_initializer(value=0, dtype=tf.float32)：常量初始化

tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)：正态分布初始化

tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)：截取的正态分布初始化

tf.random_uniform_initializer(minval=0, maxval=None, seed=None, dtype=tf.float32)：均匀分布初始化

tf.zeros_initializer(shape, dtype=tf.float32, partition_info=None)：全0常量初始化

tf.ones_initializer(dtype=tf.float32, partition_info=None)：全1常量初始化

tf.uniform_unit_scaling_initializer(factor=1.0, seed=None, dtype=tf.float32)：均匀分布（不指定最小、最大值)初始化

tf.variance_scaling_initializer(scale = 1.0, mode = "fan_in", distribution = "normal", seed = None, dtype = dtypes.float32)：由mode确定数量的截取的正态分布或均匀分

tf.orthogonal_initializer(gain=1.0, dtype=tf.float32, seed=None)：正交矩阵初始化

tf.glorot_uniform_initializer(seed=None, dtype=tf.float32)：由输入单元节点数和输出单元节点数确定的均匀分布初始化

tf.glorot_normal_initializer(seed=None, dtype=tf.float32)：由输入单元节点数和输出单元节点数确定的截取的正态分布初始化

note: tf.get_variable中initializer的初始化不需要指定shape了，已在外面指定
```


# tf.Variable的使用方法

```
tf.Variable(initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None)
```

initial_value为初始化变量的值，有以下几种方法

```
tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)

tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)

tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)

tf.random_shuffle(value, seed=None, name=None)

tf.random_crop(value, size, seed=None, name=None)

tf.multinomial(logits, num_samples, seed=None, name=None)

tf.random_gamma(shape, alpha, beta=None, dtype=tf.float32, seed=None, name=None)

tf.set_random_seed(seed) 设置产生随机数的种子

tf.zeros(shape, dtype=tf.float32, name=None)

tf.zeros_like(tensor, dtype=None, name=None)

tf.ones(shape, dtype=tf.float32, name=None)

tf.ones_like(tensor, dtype=None, name=None)

tf.fill(dims, value, name=None)

tf.constant(value, dtype=None, shape=None, name='Const')
```

# 区别

使用tf.Variable时，如果检测到命名冲突，系统会自己处理。使用tf.get_variable()时，系统不会处理冲突，而会报错

# scope 

在训练深度网络时，为了减少需要训练参数的个数、或是多机多卡并行化训练大数据大模型（比如数据并行化）等情况时，往往需要共享变量。另外一方面是当一个深度学习模型变得非常复杂的时候，往往存在大量的变量和操作，如何避免这些变量名和操作名的唯一不重复，同时维护一个条理清晰的graph非常重要。因此，tensorflow中用tf.Variable(),tf.get_variable(),tf.Variable_scope(),tf.name_scope()几个函数来实现：

## 1、tf.Variable(<variable_name>),tf.get_variable(<variable_name>)的作用与区别：

tf.Variable(<variable_name>)会自动检测命名冲突并自行处理，但tf.get_variable(<variable_name>)则遇到重名的变量创建且变量名没有设置为共享变量时，则会报错。

tf.Variable(<variable_name>)和tf.get_variable(<variable_name>)都是用于在一个name_scope下面获取或创建一个变量的两种方式，区别在于：

tf.Variable(<variable_name>)用于创建一个新变量，在同一个name_scope下面，可以创建相同名字的变量，底层实现会自动引入别名机制，两次调用产生了其实是两个不同的变量。

tf.get_variable(<variable_name>)用于获取一个变量，并且不受name_scope的约束。当这个变量已经存在时，则自动获取；如果不存在，则自动创建一个变量。

## 2、tf.name_scope(<scope_name>)与tf.variable_scope(<scope_name>)：

tf.name_scope(<scope_name>):主要用于管理一个图里面的各种op，返回的是一个以scope_name命名的context manager。一个graph会维护一个name_space的堆，每一个namespace下面可以定义各种op或者子namespace，实现一种层次化有条理的管理，避免各个op之间命名冲突。

tf.variable_scope(<scope_name>)：一般与tf.name_scope()配合使用，用于管理一个graph中变量的名字，避免变量之间的命名冲突，tf.variable_scope(<scope_name>)允许在一个variable_scope下面共享变量。

需要注意的是：创建一个新的variable_scope时不需要把reuse属性设置未False，只需要在使用的时候设置为True就可以了。

```
with tf.variable_scope("image_filters1") as scope1:
    result1 = my_image_filter(image1)
with tf.variable_scope(scope1, reuse = True)
   result2 = my_image_filter(image2)
```

scopes have the same effect on all operations as well as variables created using tf.Variable, i.e. the scope will be added as a prefix to the operation or variable name. However, name scope is ignored by tf.get_variable. We can see that in the following example:

```
with tf.name_scope("my_scope"):
    #tf.get_variable将忽略name_scope对命名空间的影响
    v1 = tf.get_variable("var1", [1], dtype=tf.float32)
    v2 = tf.Variable(1, name="var2", dtype=tf.float32)
    a = tf.add(v1, v2)

print(v1.name)  # var1:0
print(v2.name)  # my_scope/var2:0
print(a.name)   # my_scope/Add:0
```

```
with tf.variable_scope("my_scope"):
    v1 = tf.get_variable("var1", [1], dtype=tf.float32)
    v2 = tf.Variable(1, name="var2", dtype=tf.float32)
    a = tf.add(v1, v2)

print(v1.name)  # my_scope/var1:0
print(v2.name)  # my_scope/var2:0
print(a.name)   # my_scope/Add:0
```